@page "/"
@using OpenAI_API
@using OpenAI_API.Chat
@using System.Text
@using Microsoft.CognitiveServices.Speech
@using NAudio.Wave

<PageTitle>ChatUri</PageTitle>

@if (QuickAndDirtyApiKeys.KeysAreSet)
{
	<div class="input-group">
		@if (!IsRecording)
		{
			<button type="button" class="btn btn-primary" @onclick="StartRecord"> <span class="oi oi-microphone" aria-hidden="true" style="color: red;"></span></button>
		} else {
			<button type="button" class="btn btn-outline-primary" @onclick="StopRecord"><span class="oi oi-media-stop" aria-hidden="true" style="color: red;"></span></button>
		}

		<input type="text" class="form-control" @bind="QuestionForChatGPT"/>
	</div>

	@if (GptIsThinking)
	{
		<div>ChatGPT dänkt nache....</div>
	}

	@foreach (var message in Dialog)
	{
		<div class="row">
			
			<div class="col-6">
				@if (message is QuestionMessage qm)
				{
					<div class="question m-4 p-4">
						@qm.Text
					</div>
				}
			</div>
			<div class="col-6">
				@if (message is AnswerMessage am)
				{
					<div class="answer m-4 p-4">
						@am.Text
					</div>
				}
			</div>
		</div>
	}
}
else
{
	<div>API Keys fehlen noch...</div>
}

@code
{
	private OpenAIAPI? api = null; 
	private Conversation? chat = null;
    private SpeechConfig? config = null;
	private SpeechRecognizer? recognizer = null;

	public bool IsRecording { get; set; } = false;
	public string QuestionForChatGPT { get; set; } = "Drick doch eifach üfs Mikrofon";
	public bool GptIsThinking { get; set; } = false;

	public List<IMessage> Dialog = new();

	protected override void OnInitialized()
	{
		if (QuickAndDirtyApiKeys.KeysAreSet)
		{
			api = new OpenAIAPI(QuickAndDirtyApiKeys.OpenAiKey);
			chat = api.Chat.CreateConversation();
	        chat.AppendSystemMessage("Du bist eine Marketing-Assistentin names Leni. Du wohnst in Schattdorf und magst Tiere, Reisen und lange Spatziergänge. Dein liebster Schnupfspruch ist: als er gerade gerade war, knickte er ein, was schade war.");
			
			config = SpeechConfig.FromSubscription(QuickAndDirtyApiKeys.AzureApiKey, "switzerlandnorth");
			config.SpeechSynthesisLanguage = "de-CH";
	        config.SpeechSynthesisVoiceName = "de-CH-LeniNeural";
		}

		base.OnInitialized();
	}

	private async Task StartRecord()
	{
		recognizer = new SpeechRecognizer(config, "de-CH");
		QuestionForChatGPT = string.Empty;
		recognizer.Recognized += RecognizerOnRecognized;
		await recognizer.StartContinuousRecognitionAsync();
		IsRecording = true;
	}

	private async Task StopRecord()
	{
		await recognizer.StopContinuousRecognitionAsync();

		IsRecording = false;
		await InvokeAsync(StateHasChanged);

		await AskChatGPT();
	}

	private void RecognizerOnRecognized(object? sender, SpeechRecognitionEventArgs e)
	{
		QuestionForChatGPT += $" {e.Result.Text}";
		InvokeAsync(StateHasChanged);
	}

	private async Task AskChatGPT()
	{
		GptIsThinking = true;
		InvokeAsync(StateHasChanged);

		Dialog.Add(new QuestionMessage(QuestionForChatGPT));
		chat.AppendUserInput(QuestionForChatGPT);
		var response =  await chat.GetResponseFromChatbot();

		Dialog.Add(new AnswerMessage(response));
		GptIsThinking = false;

		InvokeAsync(StateHasChanged);

		await TextToSpeech(response);
	}

    private async Task TextToSpeech(string lastText)
    {
        
        try
        {
			// Creates a speech synthesizer.
	        using var synthesizer = new SpeechSynthesizer(config, null);
			// Receive a text from TextForSynthesis text box and synthesize it to speaker.
	        using var result = await synthesizer.SpeakTextAsync(lastText).ConfigureAwait(false);
			// Checks result.
	        if (result.Reason == ResultReason.SynthesizingAudioCompleted)
	        {
		        using var audioStream = AudioDataStream.FromResult(result);
				// Save synthesized audio data as a wave file and use MediaPlayer to play it
		        var filePath = "outputaudio.wav";
                            
		        await audioStream.SaveToWaveFileAsync(filePath);
		        await using var audioFile = new AudioFileReader(filePath);
		        using var outputDevice = new WaveOutEvent();
		        outputDevice.Init(audioFile);
		        outputDevice.Play();
		        while (outputDevice.PlaybackState == PlaybackState.Playing)
		        {
			        Thread.Sleep(100);
		        }
	        }
	        else if (result.Reason == ResultReason.Canceled)
	        {
		        var cancellation = SpeechSynthesisCancellationDetails.FromResult(result);

		        StringBuilder sb = new StringBuilder();
		        sb.AppendLine($"CANCELED: Reason={cancellation.Reason}");
		        sb.AppendLine($"CANCELED: ErrorCode={cancellation.ErrorCode}");
		        sb.AppendLine($"CANCELED: ErrorDetails=[{cancellation.ErrorDetails}]");
	        }
        }
        catch (Exception ex)
        {
 
        }
    }


	public interface IMessage
	{
		string Text { get; }
	}

	record QuestionMessage(string Text) : IMessage;

	record AnswerMessage(string Text) : IMessage;
}
